{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换数据格式-tfrecords：\n",
    "------------------------\n",
    "1、tfrecords是一种序列化数据格式，适用于tensorflow框架中模型训练使用\n",
    "\n",
    "2、转换成tfrecords数据格式，具体数据格式如下：\n",
    "\n",
    "```\n",
    "Example{\n",
    "    \"feature\":tf.init64_list,\n",
    "    \"label\":tf.float_list\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 11 15:22:37 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN V             Off  | 00000000:3F:00.0 Off |                  N/A |\n",
      "| 28%   32C    P2    35W / 250W |  11962MiB / 12066MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def get_tfrecords_example(feature, label):\n",
    "    tfrecords_features = {\n",
    "        'feature': tf.train.Feature(int64_list=tf.train.Int64List(value=feature)),\n",
    "        'label': tf.train.Feature(float_list=tf.train.FloatList(value=label))\n",
    "    }\n",
    "    \n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(feature=tfrecords_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process To tfrecord File: ./save_data/train_set_tohash ...\n",
      "Process To tfrecord File: ./save_data/train_set_tohash End\n",
      "Process To tfrecord File: ./save_data/test_set_tohash ...\n"
     ]
    }
   ],
   "source": [
    "def to_tfrecords(file, save_dir):\n",
    "    print(\"Process To tfrecord File: %s ...\" % file)\n",
    "    num = 0\n",
    "    writer = tf.io.TFRecordWriter(save_dir + \"/\" + \"part-0000\" + str(num) + \".tfrecords\")\n",
    "    lines = open(file)\n",
    "    for i, line in enumerate(lines):\n",
    "        tmp = line.strip().split(\",\")\n",
    "        feature = [int(tmp[0]), int(tmp[1])]\n",
    "        label = [float(1) if float(tmp[2]) >= 3 else float(0)]\n",
    "        example = get_tfrecords_example(feature, label)\n",
    "        writer.write(example.SerializeToString())\n",
    "        if (i+1) % 200000 == 0:\n",
    "            writer.close()\n",
    "            num += 1\n",
    "            writer = tf.io.TFRecordWriter(save_dir + \"/\" + \"part-0000\" + str(num) + \".tfrecords\")\n",
    "    print(\"Process To tfrecord File: %s End\" % file)\n",
    "    writer.close()\n",
    "    \n",
    "train_file_path = './save_data/train_set_tohash'\n",
    "train_totfrecord = './save_data/train'\n",
    "test_file_path = './save_data/test_set_tohash'\n",
    "test_totfrecord = './save_data/val'\n",
    "\n",
    "os.mkdir(train_totfrecord)\n",
    "os.mkdir(test_totfrecord)\n",
    "to_tfrecords(train_file_path, train_totfrecord)\n",
    "to_tfrecords(test_file_path, test_totfrecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型代码开发架构图\n",
    "------------------------\n",
    "\n",
    "PS(Parameter Server)\n",
    "------------------------\n",
    "```\n",
    "Key1, vector1\n",
    "Key2, vector2\n",
    "Key3, vector3\n",
    "...\n",
    "KeyN, vectorN\n",
    "```\n",
    "简化版Parameter Server，用一个字典进行存储，不是一开始就对所有的特征向量随机化，而是用到了才随机产生\n",
    "\n",
    "Input层\n",
    "------------------------\n",
    "读取tfrecords数据，并从PS(参数服务)中取出对应的向量，构建完整的input层\n",
    "\n",
    "Data1{\n",
    "    feature->[batch, featureNum].\n",
    "    label->[1]\n",
    "}\n",
    "\n",
    "Data1{\n",
    "    feature->[batch, featureNum, vector_len],\n",
    "    label->[1]\n",
    "}\n",
    "\n",
    "\n",
    "模型层\n",
    "------------------------\n",
    "模型多轮训练\n",
    "$$\\mathrm{M}^{\\prime} \\mathrm{UI}=\\sum_{k=1}^{K} P_{U, k} Q_{k, I}$$\n",
    "$$S S E=E^{2}=\\sum_{U, \\mathrm{I}}\\left(M_{U, I}-M_{U, l}^{\\prime}\\right)^{2}$$\n",
    "\n",
    "\n",
    "参数更新SGD\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS(Parameter Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metaclass=Singleton\n",
    "class PS:\n",
    "    def __init__(self, embedding_dim):\n",
    "        np.random.seed(2020)\n",
    "        self.params_server = dict()\n",
    "        self.dim = embedding_dim\n",
    "        print(\"ps inited...\")\n",
    "        \n",
    "    def pull(self, keys):  # 从参数服务器拉去特征所对应的参数\n",
    "        values = []\n",
    "        # 这里传进来的数据是[batch, feature_len]->一个样本的数据，样本的特征长度\n",
    "        for k in keys:\n",
    "            tmp = []\n",
    "            for arr in k:\n",
    "                value = self.params_server.get(arr, None)\n",
    "                if value is None:\n",
    "                    value = np.random.rand(self.dim)\n",
    "                    self.params_server[arr] = value\n",
    "                tmp.append(value)\n",
    "            values.append(tmp)\n",
    "        \n",
    "        return np.asarray(values, dtype='float32')\n",
    "    \n",
    "    def push(self, keys, values):\n",
    "        for i in range(len(keys)):\n",
    "            for j in range(len(keys[i])):  # [batch, feature_len]\n",
    "                self.params_server[keys[i][j]] = values[i][j]\n",
    "    \n",
    "    \n",
    "    def delete(self, keys):\n",
    "        for k in keys:\n",
    "            self.params_server.pop(k)\n",
    "            \n",
    "    def save(self, path):\n",
    "        print(\"总共包含keys： \", len(self.params_server))\n",
    "        writer = open(path, \"w\")\n",
    "        for k, v in self.params_server.items():\n",
    "            writer.write(str(k) + \"\\t\" + \",\".join([\"%.8f\" % _ for _ in v]) + \"\\n\")\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 测试PS各个功能\n",
    "    ps_local = PS(8)\n",
    "    keys = [[123, 234], [567, 891]]\n",
    "    # 从参数服务pull keys, 如果参数服务中有这个key就直接去除，若没有就随机初始取出\n",
    "    res = ps_local.pull(keys)\n",
    "    print(\"参数服务器中有哪些参数：\\n\", ps_local.params_server)\n",
    "    print(\"keys获取对应的向量：\\n\", res)\n",
    "    \n",
    "    # 经过多轮迭代更新后，传入参数服务器中\n",
    "    gradient = 10\n",
    "    res = res - 0.01 * gradient\n",
    "    ps_local.push(keys, res)\n",
    "    print(\"经过多轮迭代更新后，参数服务器中的参数:\\n\", ps_local.params_server)\n",
    "    \n",
    "    # 经过上述多乱的pull参数，然后梯度更新后，获得最终的key对应的向量embedding\n",
    "    # 保存向量，该向量用于召回\n",
    "    path = './save_data/feature_embedding/test_embedding'\n",
    "    ps_local.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFn:\n",
    "    \n",
    "    def __init__(self, local_ps):\n",
    "        self.feature_len = 2\n",
    "        self.label_len = 1\n",
    "        self.n_parse_threads = 4\n",
    "        self.shuffle_buffer_size = 1024\n",
    "        self.prefetch_buffer_size = 1\n",
    "        self.batch = 8\n",
    "        self.local_ps = local_ps\n",
    "        \n",
    "    def input_fn(self, data_dir, is_test=False):\n",
    "        def _parse_example(example):\n",
    "            features = {\n",
    "                \"feature\": tf.io.FixedLenFeature(self.feature_len, tf.int64),\n",
    "                \"label\": tf.io.FixedLenFeature(self.label_len, tf.float32),\n",
    "            }\n",
    "            return tf.io.parse_single_example(example, features)\n",
    "        \n",
    "        def _get_embedding(parsed):\n",
    "            keys = parsed[\"feature\"]\n",
    "            keys_array = tf.compat.v1.py_func(self.local_ps.pull, [keys], tf.float32)\n",
    "            result = {\n",
    "                \"feature\": parsed[\"feature\"],\n",
    "                \"label\": parsed[\"label\"]\n",
    "            }\n",
    "            return tf.io.parse_single_example(example, features)\n",
    "        \n",
    "        def _get_embedding(parsed):\n",
    "            keys = parsed[\"feature\"]\n",
    "            keys_array = tf.compat.v1.py_func(self.local_ps.pull, [keys], tf.float32)\n",
    "            result = {\n",
    "                \"feature\": parsed[\"feature\"],\n",
    "                \"label\": parsed[\"label\"],\n",
    "                \"feature_embedding\": keys_array,\n",
    "            }\n",
    "            return result\n",
    "        \n",
    "        file_list = os.listdir(data_dir)\n",
    "        files = []\n",
    "        for i in range(len(file_list)):\n",
    "            files.append(os.path.join(data_dir, file_list[i]))\n",
    "        \n",
    "        dataset = tf.compat.v1.data.Dataset.list_files(files)\n",
    "        # 数据复制多少份\n",
    "        if is_test:\n",
    "            dataset = dataset.repeat(1)\n",
    "        else:\n",
    "            dataset = dataset.repeat()\n",
    "        # 读取tfrecord数据\n",
    "        dataset = dataset.interleave(\n",
    "            lambda _: tf.compat.v1.data.TFRecordDataset(_),\n",
    "            cycle_length=1\n",
    "        )\n",
    "        # 对tfrecord的数据进行解析\n",
    "        dataset = dataset.map(\n",
    "            _parse_example,\n",
    "            num_parallel_calls=self.n_parse_threads)\n",
    "        \n",
    "        # batch data\n",
    "        dataset = dataset.batch(\n",
    "            self.batch, drop_remainder=True)\n",
    "        \n",
    "        dataset = dataset.map(\n",
    "            _get_embedding,\n",
    "            num_parallel_calls=self.n_parse_threads)\n",
    "        \n",
    "        # 对数据进行打乱\n",
    "        if not is_test:\n",
    "            dataset.shuffle(self.shuffle_buffer_size)\n",
    "            \n",
    "        # 数据预加载\n",
    "        dataset = dataset.prefetch(\n",
    "            buffer_size=self.prefetch_buffer_size)\n",
    "        \n",
    "        # 迭代器\n",
    "        iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n",
    "        return iterator, iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    local_ps = PS(8)\n",
    "    inputs = InputFn(local_ps)\n",
    "    data_dir = './save_data/train/'\n",
    "    train_itor, train_inputs = inputs.input_fn(data_dir, is_test=False)\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(train_itor.initializer)\n",
    "        for i in range(1):\n",
    "            print(sess.run(train_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
