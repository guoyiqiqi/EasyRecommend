{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def get_tfrecords_example(feature, label):\n",
    "    tfrecords_features = {\n",
    "        'feature': tf.train.Feature(int64_list=tf.train.Int64List(value=feature)),\n",
    "        'label': tf.train.Feature(float_list=tf.train.FloatList(value=label))\n",
    "    }\n",
    "    \n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(feature=tfrecords_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tfrecords(file, save_dir):\n",
    "    print(\"Process To tfrecord File: %s ...\" % file)\n",
    "    num = 0\n",
    "    writer = tf.io.TFRecordWriter(save_dir + \"/\" + \"part-0000\" + str(num) + \".tfrecords\")\n",
    "    lines = open(file)\n",
    "    for i, line in enumerate(lines):\n",
    "        tmp = line.strip().split(\",\")\n",
    "        feature = [int(tmp[0]), int(tmp[1])]\n",
    "        label = [float(1) if float(tmp[2]) >= 3 else float(0)]\n",
    "        example = get_tfrecords_example(feature, label)\n",
    "        writer.write(example.SerializeToString())\n",
    "        if (i+1) % 200000 == 0:\n",
    "            writer.close()\n",
    "            num += 1\n",
    "            writer = tf.io.TFRecordWriter(save_dir + \"/\" + \"part-0000\" + str(num) + \".tfrecords\")\n",
    "    print(\"Process To tfrecord File: %s End\" % file)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metaclass=Singleton\n",
    "class PS:\n",
    "    def __init__(self, embedding_dim):\n",
    "        np.random.seed(2020)\n",
    "        self.params_server = dict()\n",
    "        self.dim = embedding_dim\n",
    "        print(\"ps inited...\")\n",
    "        \n",
    "    def pull(self, keys):  # 从参数服务器拉去特征所对应的参数\n",
    "        values = []\n",
    "        # 这里传进来的数据是[batch, feature_len]->一个样本的数据，样本的特征长度\n",
    "        for k in keys:\n",
    "            tmp = []\n",
    "            for arr in k:\n",
    "                value = self.params_server.get(arr, None)\n",
    "                if value is None:\n",
    "                    value = np.random.rand(self.dim)\n",
    "                    self.params_server[arr] = value\n",
    "                tmp.append(value)\n",
    "            values.append(tmp)\n",
    "        \n",
    "        return np.asarray(values, dtype='float32')\n",
    "    \n",
    "    def push(self, keys, values):\n",
    "        for i in range(len(keys)):\n",
    "            for j in range(len(keys[i])):  # [batch, feature_len]\n",
    "                self.params_server[keys[i][j]] = values[i][j]\n",
    "    \n",
    "    \n",
    "    def delete(self, keys):\n",
    "        for k in keys:\n",
    "            self.params_server.pop(k)\n",
    "            \n",
    "    def save(self, path):\n",
    "        print(\"总共包含keys： \", len(self.params_server))\n",
    "        writer = open(path, \"w\")\n",
    "        for k, v in self.params_server.items():\n",
    "            writer.write(str(k) + \"\\t\" + \",\".join([\"%.8f\" % _ for _ in v]) + \"\\n\")\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFn:\n",
    "    \n",
    "    def __init__(self, local_ps):\n",
    "        self.feature_len = 2\n",
    "        self.label_len = 1\n",
    "        self.n_parse_threads = 4\n",
    "        self.shuffle_buffer_size = 1024\n",
    "        self.prefetch_buffer_size = 1\n",
    "        self.batch = 8\n",
    "        self.local_ps = local_ps\n",
    "        \n",
    "    def input_fn(self, data_dir, is_test=False):\n",
    "        def _parse_example(example):\n",
    "            features = {\n",
    "                \"feature\": tf.io.FixedLenFeature(self.feature_len, tf.int64),\n",
    "                \"label\": tf.io.FixedLenFeature(self.label_len, tf.float32),\n",
    "            }\n",
    "            return tf.io.parse_single_example(example, features)\n",
    "        \n",
    "        def _get_embedding(parsed):\n",
    "            keys = parsed[\"feature\"]\n",
    "            keys_array = tf.compat.v1.py_func(self.local_ps.pull, [keys], tf.float32)\n",
    "            result = {\n",
    "                \"feature\": parsed[\"feature\"],\n",
    "                \"label\": parsed[\"label\"]\n",
    "            }\n",
    "            return tf.io.parse_single_example(example, features)\n",
    "        \n",
    "        def _get_embedding(parsed):\n",
    "            keys = parsed[\"feature\"]\n",
    "            keys_array = tf.compat.v1.py_func(self.local_ps.pull, [keys], tf.float32)\n",
    "            result = {\n",
    "                \"feature\": parsed[\"feature\"],\n",
    "                \"label\": parsed[\"label\"],\n",
    "                \"feature_embedding\": keys_array,\n",
    "            }\n",
    "            return result\n",
    "        \n",
    "        file_list = os.listdir(data_dir)\n",
    "        files = []\n",
    "        for i in range(len(file_list)):\n",
    "            files.append(os.path.join(data_dir, file_list[i]))\n",
    "        \n",
    "        dataset = tf.compat.v1.data.Dataset.list_files(files)\n",
    "        # 数据复制多少份\n",
    "        if is_test:\n",
    "            dataset = dataset.repeat(1)\n",
    "        else:\n",
    "            dataset = dataset.repeat()\n",
    "        # 读取tfrecord数据\n",
    "        dataset = dataset.interleave(\n",
    "            lambda _: tf.compat.v1.data.TFRecordDataset(_),\n",
    "            cycle_length=1\n",
    "        )\n",
    "        # 对tfrecord的数据进行解析\n",
    "        dataset = dataset.map(\n",
    "            _parse_example,\n",
    "            num_parallel_calls=self.n_parse_threads)\n",
    "        \n",
    "        # batch data\n",
    "        dataset = dataset.batch(\n",
    "            self.batch, drop_remainder=True)\n",
    "        \n",
    "        dataset = dataset.map(\n",
    "            _get_embedding,\n",
    "            num_parallel_calls=self.n_parse_threads)\n",
    "        \n",
    "        # 对数据进行打乱\n",
    "        if not is_test:\n",
    "            dataset.shuffle(self.shuffle_buffer_size)\n",
    "            \n",
    "        # 数据预加载\n",
    "        dataset = dataset.prefetch(\n",
    "            buffer_size=self.prefetch_buffer_size)\n",
    "        \n",
    "        # 迭代器\n",
    "        iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n",
    "        return iterator, iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch = 32\n",
    "embedding_dim = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "def mf_fn(inputs, is_test):\n",
    "    # 取特征和y值，feature为：user_id和movie_id\n",
    "    embed_layer = inputs[\"feature_embedding\"]  # [batch, 2, embedding_dim]\n",
    "    embed_layer = tf.reshape(embed_layer, shape=[-1, 2, embedding_dim])\n",
    "    label = inputs[\"label\"]  # [batch, 1]\n",
    "    # 切分数据，获得user_id的embedding和movie_id的embedding\n",
    "    embed_layer = tf.split(embed_layer, num_or_size_splits=2, axis=1)\n",
    "    user_id_embedding = tf.reshape(embed_layer[0], shape=[-1, embedding_dim])\n",
    "    movie_id_embedding = tf.reshape(embed_layer[1], shape=[-1, embedding_dim])\n",
    "    # 根据公式进行乘积并求和\n",
    "    out_ = tf.reduce_mean(\n",
    "        user_id_embedding * movie_id_embedding, axis=1)  # [batch]\n",
    "    # 设定预估部分\n",
    "    out_tmp = tf.sigmoid(out_)   # batch\n",
    "    if is_test:\n",
    "        tf.compat.v1.add_to_collections(\"input_tensor\", embed_layer)\n",
    "        tf.compat.v1.add_to_collections(\"output_tensor\", out_tmp)\n",
    "        \n",
    "    # 损失函数loss\n",
    "    label_ = tf.reshape(label, [-1]) # [batch]\n",
    "    loss_ = tf.reduce_sum(tf.square(label_ - out_)) # 1\n",
    "    \n",
    "    out_dic = {\n",
    "        \"loss\": loss_,\n",
    "        \"ground_truth\": label_,\n",
    "        \"prediction\": out_\n",
    "    }\n",
    "    \n",
    "    return out_dic\n",
    "\n",
    "# 定义整个图结构，并给出梯度更新方式\n",
    "def setup_graph(inputs, is_test=False):\n",
    "    result = {}\n",
    "    with tf.compat.v1.variable_scope(\"net_graph\", reuse=is_test):\n",
    "        # 初始模型图\n",
    "        net_out_dic = mf_fn(inputs, is_test)\n",
    "        \n",
    "        loss = net_out_dic[\"loss\"]\n",
    "        result[\"out\"] = net_out_dic\n",
    "        \n",
    "        if is_test:\n",
    "            return result\n",
    "        \n",
    "        # SGD\n",
    "        emb_grad = tf.gradients(\n",
    "            loss, [inputs[\"feature_embedding\"]], name=\"feature_embedding\")[0]\n",
    "        \n",
    "        result[\"feature_new_embedding\"] = \\\n",
    "            inputs[\"feature_embedding\"] - learning_rate * emb_grad\n",
    "        \n",
    "        result[\"feature_embedding\"] = inputs[\"feature_embedding\"]\n",
    "        result[\"feature\"] = inputs[\"feature\"]\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class AUCUtil(object):\n",
    "    \"\"\"Summary\n",
    "    Args:\n",
    "        ground_truth(list):\n",
    "        loss(list):\n",
    "        prediction(list):\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def add(self, loss, g=np.array([]), p=np.array([])):\n",
    "        self.loss.append(loss)\n",
    "        self.ground_truth += g.flatten().tolist()\n",
    "        self.prediction += p.flatten().tolist()\n",
    "        \n",
    "    def calc(self):\n",
    "        return {\n",
    "            \"loss_num\": len(self.loss),\n",
    "            \"loss\": np.array(self.loss).mean(),\n",
    "            \"auc_num\": len(self.ground_truth),\n",
    "            \"auc\": roc_auc_score(self.ground_truth, self.prediction) if len(self.ground_truth) > 0 else 0,\n",
    "            \"pcoc\": sum(self.prediction) / sum(self.ground_truth)\n",
    "        }\n",
    "    \n",
    "    def calc_str(self):\n",
    "        res = self.calc()\n",
    "        return \"loss: %f(%d), auc: %f(%d), pcoc: %f\" % (\n",
    "            res[\"loss\"], res[\"loss_num\"],\n",
    "            res[\"auc\"], res[\"auc_num\"],\n",
    "            res[\"pcoc\"]\n",
    "        )\n",
    "    \n",
    "    def reset(self):\n",
    "        self.loss = []\n",
    "        self.ground_truth = []\n",
    "        self.prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps inited...\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/basicenv/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py:332: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "batch = 32\n",
    "embedding_dim = 8\n",
    "local_ps = PS(embedding_dim)\n",
    "n_parse_threads = 4\n",
    "shuffle_buffer_size = 1024\n",
    "prefetch_buffer_size = 16\n",
    "max_steps = 100000\n",
    "test_show_step = 1000\n",
    "# 数据输入\n",
    "inputs = InputFn(local_ps)\n",
    "\n",
    "last_test_auc = 0.\n",
    "# 训练\n",
    "train_metric = AUCUtil()\n",
    "test_metric = AUCUtil()\n",
    "\n",
    "train_file = './save_data/train/'\n",
    "test_file = './save_data/val/'\n",
    "\n",
    "saved_embedding = ''\n",
    "\n",
    "train_itor, train_inputs = inputs.input_fn(train_file, is_test=False)\n",
    "train_dic = setup_graph(train_inputs, is_test=False)\n",
    "\n",
    "test_itor, test_inputs = inputs.input_fn(test_file, is_test=True)\n",
    "test_dic = setup_graph(test_inputs, is_test=True)\n",
    "\n",
    "train_log_iter = 1000\n",
    "last_test_auc = 0.5\n",
    "\n",
    "def train():\n",
    "    _step = 0\n",
    "    print(\"#\" * 80)\n",
    "    # 建立sess,进行训练\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # init global & local variables\n",
    "        sess.run([tf.compat.v1.global_variables_initializer(),\n",
    "                  tf.compat.v1.local_variables_initializer()])\n",
    "        # 开始训练\n",
    "        sess.run(train_itor.initializer)\n",
    "        while _step < max_steps:\n",
    "            feature_old_embedding, feature_new_embedding, keys, out = sess.run(\n",
    "                [train_dic[\"feature_embedding\"],\n",
    "                 train_dic[\"feature_new_embedding\"],\n",
    "                 train_dic[\"feature\"],\n",
    "                 train_dic[\"out\"]]\n",
    "            )\n",
    "            \n",
    "            train_metric.add(\n",
    "                out[\"loss\"],\n",
    "                out[\"ground_truth\"],\n",
    "                out[\"prediction\"])\n",
    "            \n",
    "            local_ps.push(keys, feature_new_embedding)\n",
    "            _step += 1\n",
    "            \n",
    "            # 每次训练多少个batch的训练数，就打印一次训练的这些batch的auc的信息\n",
    "            if _step % train_log_iter == 0:\n",
    "                print(\"Train at step %d: %s\", _step, train_metric.calc_str())\n",
    "                train_metric.reset()\n",
    "            if _step % test_show_step == 0:\n",
    "                valid_step(sess, test_itor, test_dic)\n",
    "\n",
    "def valid_step(sess, test_itor, test_dic):\n",
    "    test_metric.reset()\n",
    "    sess.run(test_itor.initializer)\n",
    "    global last_test_auc\n",
    "    while True:\n",
    "        try:\n",
    "            out = sess.run(test_dic[\"out\"])\n",
    "            \n",
    "            test_metric.add(\n",
    "                out[\"loss\"],\n",
    "                out[\"ground_truth\"],\n",
    "                out[\"prediction\"])\n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Test at step: %s\", test_metric.calc_str())\n",
    "            if test_metric.calc()[\"auc\"] > last_test_auc:\n",
    "                last_test_auc = test_metric.calc()[\"auc\"]\n",
    "                local_ps.save(saved_embedding)\n",
    "                \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Train at step %d: %s 1000 loss: 3.842416(1000), auc: 0.502888(8000), pcoc: 0.303597\n",
      "Test at step: %s loss: 3.841778(1200823), auc: 0.498068(9606584), pcoc: 0.304140\n",
      "Train at step %d: %s 2000 loss: 3.811954(1000), auc: 0.492183(8000), pcoc: 0.307573\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bkdr2hash64(str01):\n",
    "    mask60 = 0x0fffffffffffffff\n",
    "    seed = 131\n",
    "    hash = 0\n",
    "    for s in str01:\n",
    "        hash = hash * seed + ord(s)\n",
    "    return hash & mask60\n",
    "\n",
    "# 读取文件\n",
    "def read_embedding_file(file):\n",
    "    dic = dict()\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            tmp = line.split(\"\\t\")\n",
    "            embedding = [float(_) for _ in tmp[1].split(\",\")]\n",
    "            dic[tmp[0]] = embedding\n",
    "    return dic\n",
    "\n",
    "def get_hash2id(file):\n",
    "    movie_dict = {}\n",
    "    user_dict = {}\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            tmp = line.split(\",\")\n",
    "            movie_dict[str(bkdr2hash64(\"UserID=\" + tmp[1]))] = tmp[1]\n",
    "            user_dict[str(bkdr2hash64(\"MovieID=\" + tmp[0]))] = tmp[0]\n",
    "    return user_dict, movie_dict\n",
    "\n",
    "def split_user_movie(embedding_file, train_file):\n",
    "    user_dict, movie_dict = get_hash2id(train_file)\n",
    "    embedding_dict = read_embedding_file(embedding_file)\n",
    "    \n",
    "    movie_embedding = {}\n",
    "    user_embedding = {}\n",
    "    \n",
    "    for k, v in embedding_dict.items():\n",
    "        m_id = movie_dict.get(k, None)\n",
    "        if m_id is not None:\n",
    "            movie_embedding[m_id] = v\n",
    "        u_id = user_dict.get(k, None)\n",
    "        if u_id is not None:\n",
    "            user_embedding[u_id] = v\n",
    "            \n",
    "    return movie_embedding, user_embedding\n",
    "\n",
    "# 用于i2i模式\n",
    "def col_sim(movie_sim_movie_file, movie_embedding):\n",
    "    wfile = open(movie_sim_movie_file, \"w\")\n",
    "    for m, vecl in movie_embedding.items():\n",
    "        sim_movie_tmp = {}\n",
    "        for n, vec2 in movie_embedding.items():\n",
    "            if m == n:\n",
    "                continue\n",
    "            sim_movie_tmp[n] = np.dot(np.asarray(vec2), np.asarray(vec1))\n",
    "            \n",
    "        sim_movie = sorted(sim_movie_tmp.items(), key=lambda _:_[1], reverse=True)\n",
    "        sim_movie = [str(_[0]) for _ in sim_movie][: 200]\n",
    "        \n",
    "        wfile.write(m + \"\\t\" + \",\".join(sim_movie) + \"\\n\")\n",
    "        \n",
    "# 用u2i模式和排序\n",
    "def write_user_movie_embeding(movie_embedding_file, user_embedding_file, movie_embedding, user_embedding):\n",
    "    wfile01 = open(movie_embedding_file, \"w\")\n",
    "    for k, v in movie_embedding.items():\n",
    "        wfile01.write(k + \"\\t\" + \",\".join([str(_) for _ in v]) + \"\\n\")\n",
    "    \n",
    "    wfile01.close()\n",
    "    wfile02 = open(user_embedding_file, \"w\")\n",
    "    for k, v in user_embedding.items():\n",
    "        wfile02.write(k + \"\\t\" + \",\".join([str(_) for _ in v]) + \"\\n\")\n",
    "    wfile02.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    embedding_file = './save_data/saved_embedding'\n",
    "    train_file = './save_data/train_set'\n",
    "    movie_embedding, user_embedding = split_user_movie(embedding_file, train_file)\n",
    "    \n",
    "    # 用于u2i模式和排序\n",
    "    movie_embedding_file = './save_data/movie_embedding_file'\n",
    "    user_embedding_file = './save_data/user_embedding_file'\n",
    "    write_user_movie_embeding(movie_embedding_file, user_embedding_file, movie_embedding, user_embedding)\n",
    "    \n",
    "    # 用于i2i模式\n",
    "    movie_embedding_file = './save_data/movie_sim_movie_file'\n",
    "    col_sim(movie_sim_movie_file, movie_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
